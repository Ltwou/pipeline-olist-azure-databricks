{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72b77752-31fb-4df2-a656-025dcd20d2eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls /mnt/olist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "787efe8a-d6f2-4b61-afc5-c67b0f0f9abc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d95dd36-27fc-475d-a3d8-ee852a34a68b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Caminho atualizado correto\n",
    "os.makedirs(\"/root/.config/kaggle\", exist_ok=True)\n",
    "\n",
    "# Conteúdo do seu kaggle.json\n",
    "kaggle_json_content = \"\"\"\n",
    "{\n",
    "  \"username\": \"luciojr\",\n",
    "  \"key\": \"ad8b74c5e48e49cdd3a3c917e8c5702d\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Grava o arquivo no local esperado pela API\n",
    "with open(\"/root/.config/kaggle/kaggle.json\", \"w\") as f:\n",
    "    f.write(kaggle_json_content)\n",
    "\n",
    "# Define permissões corretas\n",
    "os.chmod(\"/root/.config/kaggle/kaggle.json\", 0o600)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be19dd35-12f6-4201-bcda-9dda8eeac593",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Define o local no DBFS onde salvar\n",
    "local_path = \"/tmp/olist\"\n",
    "\n",
    "os.makedirs(local_path, exist_ok=True)\n",
    "\n",
    "# Baixa e extrai os arquivos no caminho\n",
    "api.dataset_download_files(\"olistbr/brazilian-ecommerce\", path=local_path, unzip=True)\n",
    "\n",
    "import shutil\n",
    "\n",
    "local_path = \"/tmp/olist\"\n",
    "target_path = \"/dbfs/mnt/olist/Bronze\"\n",
    "\n",
    "os.makedirs(target_path, exist_ok=True)\n",
    "\n",
    "for file in os.listdir(local_path):\n",
    "    shutil.copy(os.path.join(local_path, file), os.path.join(target_path, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22dfcd61-30c3-4398-942e-2557e52070ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Caminho onde estão os CSVs e onde os Parquets serão salvos\n",
    "bronze_path = \"dbfs:/mnt/olist/Bronze\"\n",
    "\n",
    "# Lista os arquivos na pasta Bronze\n",
    "files = dbutils.fs.ls(bronze_path)\n",
    "\n",
    "for file in files:\n",
    "    file_name = file.name\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        base_name = file_name.replace(\".csv\", \"\")\n",
    "        csv_path = os.path.join(bronze_path, file_name)\n",
    "        parquet_path = os.path.join(bronze_path, base_name)  \n",
    "\n",
    "        # Ler CSV\n",
    "        df = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "\n",
    "        # Salvar como Parquet (sobrescreve se já existir)\n",
    "        df.write.mode(\"overwrite\").parquet(parquet_path)\n",
    "\n",
    "        # Remover o CSV original\n",
    "        dbutils.fs.rm(csv_path, recurse=True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5075755963305677,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
